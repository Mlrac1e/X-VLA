# ------------------------------------------------------------------------------
# Copyright (c) 2022–∞, 2toINF
# Licensed under the Apache License, Version 2.0
# ------------------------------------------------------------------------------

from __future__ import annotations

from typing import Any, Dict, Tuple

import logging
import traceback

import numpy as np
import torch
import torch.nn as nn
from PIL import Image
from fastapi import FastAPI
from fastapi.responses import JSONResponse
import uvicorn
from safetensors.torch import load_file

from transformers import AutoModelForCausalLM
from components.transformer import SoftPromptedTransformer
from components.losses import EE6DLoss, JointLoss
from components.preprocessor import LanguagePreprocessor, ImagePreprocessor

LOSS_HUB = {
    "ee6d": EE6DLoss,
    "joint": JointLoss,
}


class XVLA(nn.Module):
    """
    XVLA

    Overview
    --------
    - Visual-Language Encoder (VLMEncoder): encodes multi-view images and language tokens.
    - SoftPromptedTransformer: fuses (noisy) action sequences, proprioception, time, and VLM features.
    - Loss: fixed-parameter loss (EE6DLoss or JointLoss), no runtime config objects.

    Notes
    -----
    - Gripper channel indices are taken from the selected loss class (`.GRIPPER_IDX`).
    - During training/inference, gripper channels in proprio and noisy actions are zeroed.
    - At inference, a sigmoid is applied only on gripper channels.
    """

    def __init__(
        self,
        encoder_name: str = "microsoft/Florence-2-base",
        *,
        depth: int = 24,
        hidden_size: int = 1024,
        num_heads: int = 16,
        num_actions: int = 30,
        num_domains: int = 30,
        len_soft_prompts: int = 32,
        use_hetero_proj: bool = False,
        action_mode: str = "ee6d",
    ):
        """
        Parameters
        ----------
        encoder_name : str
            Name or path passed to `VLMEncoder`.
        depth : int
            Transformer depth.
        hidden_size : int
            Transformer hidden size.
        num_heads : int
            Number of attention heads.
        num_actions : int
            Temporal length of the action sequence (T).
        num_domains : int
            Number of domains/task IDs for domain conditioning.
        len_soft_prompts : int
            Length of soft prompt tokens.
        use_hetero_proj : bool
            Whether to use heterogeneous projection heads in the transformer.
        action_mode : {"ee6d","joint"}
            Layout for actions/proprio; controls channel dimensions and loss type.
        """
        super().__init__()

        action_mode = action_mode.lower()
        assert action_mode in {"ee6d", "joint"}, "action_mode must be 'ee6d' or 'joint'"

        # Channel layout derived from mode
        self.criterion = LOSS_HUB[action_mode]()
        self.num_actions = num_actions

        # Modules
        assert encoder_name in {
            "microsoft/Florence-2-base",
            "microsoft/Florence-2-large",
        }, "Only microsoft/Florence-2-base and microsoft/Florence-2-large are supported."
        self.vlm = AutoModelForCausalLM.from_pretrained(
            encoder_name,
            low_cpu_mem_usage=True,
            torch_dtype="auto",
            trust_remote_code=True
        )

        # Remove decoder-specific components to reduce memory and ensure we only
        # use encoder pathways. Guard these in case internals change.
        if hasattr(self.vlm, "language_model"):
            lm = self.vlm.language_model
            if hasattr(lm, "model") and hasattr(lm.model, "decoder"):
                del lm.model.decoder
            if hasattr(lm, "lm_head"):
                del lm.lm_head
                
        
        self.transformer = SoftPromptedTransformer(
            hidden_size=hidden_size,
            multi_modal_input_size = self.vlm.config.projection_dim,
            depth=depth,
            num_heads=num_heads,
            mlp_ratio=4.0,
            num_domains=num_domains,
            dim_action=self.criterion.DIM_ACTION,
            dim_propio=self.criterion.DIM_ACTION,
            len_soft_prompts=len_soft_prompts,
            dim_time=32,
            max_len_seq=512,
            use_hetero_proj=use_hetero_proj,
        )
        

        # I/O preprocessors (implementations are project-specific)
        self.text_preprocessor = LanguagePreprocessor(encoder_name="microsoft/Florence-2-large")
        self.image_preprocessor = ImagePreprocessor()

        self.app: FastAPI | None = None


    # ------------------------------ utilities -------------------------------
    def forward_vlm(
        self,
        input_ids: torch.LongTensor,        # [B, L]
        pixel_values: torch.FloatTensor,    # [B, V, C, H, W]
        image_mask: torch.Tensor,           # [B, V], bool or 0/1
    ) -> Dict[str, torch.Tensor]:
        """
        Produce VLM token features and auxiliary visual tokens from multi-view inputs.

        Parameters
        ----------
        input_ids : LongTensor, shape [B, L]
            Token ids for the text prompt/instruction.
        pixel_values : FloatTensor, shape [B, V, C, H, W]
            Image batch with V views per sample.
        image_mask : Tensor, shape [B, V]
            Mask indicating which views are valid (True/1) vs padded (False/0).

        Returns
        -------
        Dict[str, Tensor]
            {
              "vlm_features": FloatTensor [B, T_enc, D],  # encoder token sequence
              "aux_visual_inputs": FloatTensor [B, (V-1)*N, D]  # flattened features for views 1..V-1
            }
        """
        B, V = pixel_values.shape[:2]

        # Flatten views, select valid images, encode
        flat_mask = image_mask.view(-1).to(torch.bool)
        flat_images = pixel_values.flatten(0, 1)                    # [B*V, C, H, W]

        num_valid = int(flat_mask.sum().item())
        assert num_valid > 0, "At least one image must be valid."

        valid_images = flat_images[flat_mask]                   # [#valid, C, H, W]
        valid_feats = self.vlm._encode_image(valid_images)    # [#valid, N, D]
        N, D = valid_feats.shape[1:]

        # Reconstruct dense [B, V, N, D] tensor
        image_features = valid_feats.new_zeros((B * V, N, D))
        image_features[flat_mask] = valid_feats
        image_features = image_features.view(B, V, N, D)        # [B, V, N, D]

        # Text embeddings
        inputs_embeds = self.vlm.get_input_embeddings()(input_ids) # [B, L, D]

        # Merge first-view visual tokens with text embeddings
        merged_embeds, attention_mask = self.vlm._merge_input_ids_with_image_features(
            image_features[:, 0],  # [B, N, D]
            inputs_embeds,         # [B, L, D]
        )

        # Run encoder to get token-level features
        enc_out = self.vlm.language_model.model.encoder(
            attention_mask=attention_mask,
            inputs_embeds=merged_embeds,
        )[0]  # [B, T_enc, D]

        # Remaining views (1..V-1) flattened as auxiliary inputs
        aux_visual_inputs = image_features[:, 1:].reshape(B, -1, D)  # [B, (V-1)*N, D]

        return {
            "vlm_features": enc_out,
            "aux_visual_inputs": aux_visual_inputs,
        }


    # ------------------------------ utilities -------------------------------
    def mask_gripper(self, proprio: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Zero gripper channels in `proprio` and `action`.

        Parameters
        ----------
        proprio : Tensor, shape [B, dim_proprio]
        action : Tensor, shape [B, T, dim_action]

        Returns
        -------
        proprio_m : Tensor
            Proprio with gripper channels zeroed.
        action_m : Tensor
            Action with gripper channels zeroed.
        """
        idx = self.criterion.GRIPPER_IDX
        proprio_m = proprio.clone()
        action_m = action.clone()
        proprio_m[..., idx] = 0.0
        action_m[..., idx] = 0.0
        return proprio_m, action_m

    # ------------------------------ training --------------------------------
    def forward(
        self,
        input_ids: torch.LongTensor,
        image_input: torch.FloatTensor,
        image_mask: torch.Tensor,
        domain_id: torch.Tensor,
        proprio: torch.Tensor,
        action: torch.Tensor,
    ) -> Dict[str, torch.Tensor]:
        """
        Training forward pass.

        Parameters
        ----------
        input_ids : LongTensor, shape [B, L]
            Token IDs for language input.
        image_input : FloatTensor, shape [B, V, C, H, W]
            Multi-view images.
        image_mask : Tensor, shape [B, V]
            Mask for views (1 = valid, 0 = pad).
        domain_id : LongTensor, shape [B]
            Domain/task IDs.
        proprio : Tensor, shape [B, dim_proprio]
            Proprioceptive state.
        action : Tensor, shape [B, T=num_actions, dim_action]
            Ground-truth action sequence.

        Returns
        -------
        Dict[str, Tensor]
            Loss dictionary (keys depend on loss type), including "total_loss".
        """
        enc = self.forward_vlm(input_ids=input_ids, pixel_values=image_input, image_mask=image_mask)

        B = input_ids.shape[0]
        # Per-sample time in (0, 1); avoid exactly 1.0
        time = (torch.rand(1, device=input_ids.device) + torch.arange(B, device=input_ids.device) / B) % (1 - 1e-5)

        # Mix GT action with noise
        action_with_noise = torch.randn_like(action) * time.view(-1, 1, 1) + action * (1 - time).view(-1, 1, 1)

        # Mask gripper channels
        proprio_m, action_with_noise_m = self.mask_gripper(proprio, action_with_noise)

        # Predict actions
        pred_action = self.transformer(
            domain_id=domain_id,
            action_with_noise=action_with_noise_m,
            t=time,
            proprio=proprio_m,
            **enc,
        )
        return self.criterion(pred_action, action)

    # ------------------------------ inference -------------------------------
    @torch.no_grad()
    def generate_actions(
        self,
        input_ids: torch.LongTensor,
        image_input: torch.FloatTensor,
        image_mask: torch.Tensor,
        domain_id: torch.Tensor,
        proprio: torch.Tensor,
        steps: int = 10,
    ) -> torch.Tensor:
        """
        Iterative denoising sampler (linear schedule).

        Parameters
        ----------
        input_ids : LongTensor, shape [B, L]
            Token IDs for language input.
        image_input : FloatTensor, shape [B, V, C, H, W]
            Multi-view images.
        image_mask : Tensor, shape [B, V]
            Mask for views (1 = valid, 0 = pad).
        domain_id : LongTensor, shape [B]
            Domain/task IDs.
        proprio : Tensor, shape [B, dim_proprio]
            Proprioceptive state.
        steps : int
            Number of denoising iterations.

        Returns
        -------
        Tensor, shape [B, T=num_actions, dim_action]
            Predicted action sequence; sigmoid applied only on gripper channels.
        """
        self.eval()
        enc = self.forward_vlm(input_ids=input_ids, pixel_values=image_input, image_mask=image_mask)

        B = input_ids.shape[0]
        device = input_ids.device
        x1 = torch.randn(B, self.num_actions, self.criterion.DIM_ACTION, device=device)
        action = torch.zeros(B, self.num_actions, self.criterion.DIM_ACTION, device=device)

        steps = max(int(steps), 1)
        for i in range(steps, 0, -1):
            t = torch.full((B,), i / steps, device=device)
            action_with_noise = x1 * t.view(-1, 1, 1) + action * (1 - t).view(-1, 1, 1)
            proprio_m, action_with_noise_m = self.mask_gripper(proprio, action_with_noise)
            action = self.transformer(
                domain_id=domain_id,
                action_with_noise=action_with_noise_m,
                t=t,
                proprio=proprio_m,
                **enc,
            )

        idx = self.criterion.GRIPPER_IDX
        action[..., idx] = torch.sigmoid(action[..., idx])
        return action

    # ------------------------------ minimal service -------------------------
    def _build_app(self):
        """
        Create a minimal FastAPI app with a single `/act` endpoint.

        Request JSON
        ------------
        language_instruction : str
            Natural language instruction.
        image0/1/2 : ndarray/list or file path
            Up to 3 images.
        proprio : array-like
            Proprioceptive vector of length `dim_proprio`.
        domain_id : int
            Domain/task ID.
        steps : int, optional
            Number of denoising steps (default 10).

        Response JSON
        -------------
        { "action": List[List[float]] }
            Predicted action sequence.
        """
        if self.app is not None:
            return

        app = FastAPI()

        @app.post("/act")
        def act(payload: Dict[str, Any]):
            try:
                self.eval()

                image_list = []
                for key in ("image0", "image1", "image2"):
                    if key in payload:
                        v = payload[key]
                        if isinstance(v, (list, np.ndarray)):
                            image_list.append(Image.fromarray(np.array(v)))
                        else:
                            image_list.append(Image.open(v))

                language_inputs = self.text_preprocessor.encode_language(
                    [payload["language_instruction"]]
                )
                image_inputs = self.image_preprocessor(image_list)

                proprio_np = np.asarray(payload["proprio"], dtype=np.float32)
                domain_id_val = int(payload["domain_id"])

                to_cuda = (lambda t: t.cuda(non_blocking=True)) if torch.cuda.is_available() else (lambda t: t)
                inputs = {
                    **{k: to_cuda(v) for k, v in language_inputs.items()},
                    **{k: to_cuda(v) for k, v in image_inputs.items()},
                    "proprio": to_cuda(torch.tensor(proprio_np, dtype=torch.float32).unsqueeze(0)),
                    "domain_id": to_cuda(torch.tensor([domain_id_val], dtype=torch.long)),
                }

                steps = int(payload.get("steps", 10))
                action = self.generate_actions(
                    input_ids=inputs["input_ids"],
                    image_input=inputs["pixel_values"],
                    image_mask=inputs["image_mask"],
                    domain_id=inputs["domain_id"],
                    proprio=inputs["proprio"],
                    steps=steps,
                ).squeeze(0).cpu().numpy()

                return JSONResponse({"action": action.tolist()})

            except Exception:
                logging.error(traceback.format_exc())
                msg = "Request failed; ensure payload fields are valid."
                return JSONResponse({"error": msg}, status_code=400)

        self.app = app

    def run(self, host: str = "0.0.0.0", port: int = 8000):
        """
        Launch the FastAPI service.

        Parameters
        ----------
        host : str
            Bind address.
        port : int
            Port number.
        """
        self._build_app()
        assert self.app is not None
        uvicorn.run(self.app, host=host, port=port)


def xvla(device: str = "cuda", pretrained: str | None = None, action_mode = 'ee6d', **kwargs):
    """
    Factory for an XVLA preset using Florence-2-large and a deeper transformer.

    Parameters
    ----------
    device : str
        Device to move the model to (e.g., "cuda", "cpu").
    pretrained : str | None
        Path to a safetensors checkpoint to load with `strict=False`.
    **kwargs
        Extra keyword args forwarded to `XVLA`.

    Returns
    -------
    nn.Module
        Constructed XVLA model.
    """
    model = XVLA(
        encoder_name="microsoft/Florence-2-large",
        
        depth=24,
        hidden_size=1024,
        num_heads=16,
        num_actions=30,
        num_domains=30,
        len_soft_prompts=32,
        use_hetero_proj=False,
        
        action_mode=action_mode,
    )

    if isinstance(pretrained, str):
        print(f">>>>>> load pretrain from {pretrained}")
        pretrained_ckpt = load_file(pretrained)
        print(model.load_state_dict(pretrained_ckpt, strict=False))

    if device:
        model = model.to(device)
    return model
